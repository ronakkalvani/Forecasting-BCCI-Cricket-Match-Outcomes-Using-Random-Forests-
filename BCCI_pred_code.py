# -*- coding: utf-8 -*-
"""ass3Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kwEal01QQJwnYVqW3OfC-p1R0SmhyxqN
"""

'''
This is started code for part a.
Using this code is OPTIONAL and you may write code from scratch if you want
'''

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

label_encoder = None

def get_np_array(file_name):
    global label_encoder
    data = pd.read_csv(file_name)
    need_label_encoding = ['team','host','opp','month', 'day_match']
    if(label_encoder is None):
        label_encoder = OrdinalEncoder()
        label_encoder.fit(data[need_label_encoding])
    data_1 = pd.DataFrame(label_encoder.transform(data[need_label_encoding]), columns = label_encoder.get_feature_names_out())
    dont_need_label_encoding =  ["year","toss","bat_first","format" ,"fow","score" ,"rpo" ,"result"]
    data_2 = data[dont_need_label_encoding]
    final_data = pd.concat([data_1, data_2], axis=1)
    X = final_data.iloc[:,:-1]
    y = final_data.iloc[:,-1:]
    return X.to_numpy(), y.to_numpy()

class DTNode:

    def __init__(self, depth, is_leaf = False, value = 0, column = None):
        self.depth = depth
        self.children = []
        self.is_leaf = is_leaf
        self.path = 0
        self.value = value
        self.parent = None
        if(not self.is_leaf):
            self.column = column

class DTTree:

    def __init__(self):
        self.root = None

    def fit(self, X, y, types, max_depth = 10):
        self.root = self.RecTree(0,X, y, types, max_depth)

    def RecTree(self, depth, X, y, types, max_depth):
        if depth >= max_depth or len(np.unique(y)) == 1:
            CurrNode= DTNode(depth, is_leaf= True)
            count1=0
            for i in range(len(y)):
                if y[i]==1:
                    count1+=1
            CurrNode.value=0
            if count1>=len(y)-count1:
                CurrNode.value=1
            return CurrNode
        if len(X[0])==1:
            count1=0
            for i in range(len(y)):
                if y[i]==1:
                    count1+=1
            CurrNode= DTNode(depth, is_leaf= True)
            CurrNode.value=0
            if count1>=len(y)-count1:
                CurrNode.value=1
            return CurrNode
        CurrNode= DTNode(depth, is_leaf= False)
        CurrNode.column= self.BestSplit(X,y,types)
        count1=0
        for i in range(len(y)):
            if y[i]==1:
                count1+=1
        CurrNode.value=0
        if count1>=len(y)-count1:
            CurrNode.value=1
        need= X[:,CurrNode.column]
        newX= np.hstack([X[:,:CurrNode.column],X[:,CurrNode.column+1:]])
        newtypes= np.append(types[:CurrNode.column],types[CurrNode.column+1:])
        if types[CurrNode.column]=="cat":
            labs=np.unique(need)
            for k in range(len(labs)):
                inds=np.where(need==labs[k])
                if len(inds[0])==0:
                    bachha= DTNode(depth+1, is_leaf= True, value = CurrNode.value)
                else:
                    bachha= self.RecTree(depth+1, newX[inds], y[inds], newtypes, max_depth)
                bachha.path = labs[k]
                bachha.parent = CurrNode
                CurrNode.children.append(bachha)
        else:
            med= np.median(need)
            inds=np.where(need<=med)
            if len(inds[0])==0:
                bachha= DTNode(depth+1, is_leaf= True, value = CurrNode.value)
            else:
                bachha= self.RecTree(depth+1, newX[inds], y[inds], newtypes, max_depth)
            bachha.path = med
            bachha.parent = CurrNode
            CurrNode.children.append(bachha)
            inds=np.where(need>med)
            if len(inds[0])==0:
                bachha= DTNode(depth+1, is_leaf= True, value = CurrNode.value)
            else:
                bachha= self.RecTree(depth+1, newX[inds], y[inds], newtypes, max_depth)
            bachha.path = med
            bachha.parent = CurrNode
            CurrNode.children.append(bachha)
        return CurrNode

    def BestSplit(self, X, y, types):
        mutualinfo=[0]*len(X[0])
        for i in range(len(X[0])):
            if types[i]=="cat":
                labels=np.unique(X[:,i])
                countmat=np.zeros((len(labels),2))
                point={}
                for k in range(len(labels)):
                    point[labels[k]]=k
                for j in range(len(X)):
                    if y[j]==1:
                        countmat[point[X[j,i]],1]+=1
                    else:
                        countmat[point[X[j,i]],0]+=1
                for k in range(len(labels)):
                    ProbLab=(sum(countmat[k])/len(X))
                    CondProb0= countmat[k,0]/sum(countmat[k])
                    CondProb1= 1-CondProb0
                    if CondProb0==0 or CondProb1==0:
                        mutualinfo[i]+=0
                    else:
                        CondEntropy= -(CondProb0*np.log2(CondProb0) +CondProb1*np.log2(CondProb1))
                        mutualinfo[i]+= ProbLab*CondEntropy
            else:
                meann=np.mean(X[:,i])
                countmat=np.zeros((2,2))
                for j in range(len(X)):
                    if y[j]==1:
                        if X[j,i]<meann:
                            countmat[0,1]+=1
                        else:
                            countmat[1,1]+=1
                    else:
                        if X[j,i]<meann:
                            countmat[0,0]+=1
                        else:
                            countmat[1,0]+=1
                for k in range(2):
                    if countmat[k,0]==0 or countmat[k,1]==0:
                        mutualinfo[i]+=0
                    else:
                        ProbLab=(sum(countmat[k])/len(X))
                        CondProb0= countmat[k,0]/sum(countmat[k])
                        CondProb1= 1-CondProb0
                        CondEntropy= -(CondProb0*np.log2(CondProb0) +CondProb1*np.log2(CondProb1))
                        mutualinfo[i]+= ProbLab*CondEntropy
        return np.argmin(mutualinfo)

    def PredictForThis(self,x,Curr,types):
        if Curr.is_leaf==True:
            return Curr.value
        if types[Curr.column]=="cat":
            for child in Curr.children:
                if child.path==x[Curr.column]:
                    return self.PredictForThis(np.append(x[:Curr.column],x[Curr.column+1:]),child,np.append(types[:Curr.column],types[Curr.column+1:]))
        else:
            if x[Curr.column]<=Curr.children[0].path:
                return self.PredictForThis(np.append(x[:Curr.column],x[Curr.column+1:]),Curr.children[0],np.append(types[:Curr.column],types[Curr.column+1:]))
            else:
                return self.PredictForThis(np.append(x[:Curr.column],x[Curr.column+1:]),Curr.children[1],np.append(types[:Curr.column],types[Curr.column+1:]))
        return 0

    def __call__(self, X, types):
        Prediction= np.zeros((len(X),1))
        for i in range(len(X)):
            Prediction[i][0]= self.PredictForThis(X[i],self.root,types)
        return Prediction

    def PruneRec(self,X_val,y_val,types,trav,mx,mxacc):
        if trav.is_leaf:
            return [mx,mxacc]
        for i in range(len(trav.children)):
            old = trav.children[i]
            trav.children[i] = DTNode(trav.depth+1, is_leaf= True, value = trav.value)
            child_Pred = self.__call__(X_val,types)
            ChildNode_acc = len(np.where(child_Pred[:,0]==y_val[:,0])[0])/len(child_Pred)
            trav.children[i] = old
            if mxacc<ChildNode_acc:
                mx = trav.children[i]
                mxacc = ChildNode_acc
            reccc = self.PruneRec(X_val,y_val,types,trav.children[i],mx,mxacc)
            if mxacc<reccc[1]:
                mx = reccc[0]
                mxacc = reccc[1]
        return [mx,mxacc]

    def post_prune(self, X_val, y_val,types,prun_para):
        treePred = self.__call__(X_val,types)
        treeacc = len(np.where(treePred[:,0]==y_val[:,0])[0])/len(treePred)
        print(treeacc)
        SolvePrune = self.PruneRec(X_val,y_val,types,self.root,self.root,treeacc)
        mx = SolvePrune[0]
        parrent = mx.parent
        if SolvePrune[1]-treeacc>prun_para and parrent is not None:
            for i in range(len(parrent.children)):
                if parrent.children[i].path == mx.path:
                    parrent.children[i] = DTNode(mx.depth, is_leaf= True , value = mx.value)
            print(SolvePrune[1],"---")
            self.post_prune(X_val, y_val,types,prun_para)

if __name__ == '__main__':

    X_train,y_train = get_np_array('train.csv')
    X_test, y_test = get_np_array("test.csv")
    X_val, y_val = get_np_array("val.csv")
    types = np.array(['cat','cat','cat',"cat","cat","cont","cat","cat","cat" ,"cont","cont" ,"cont" ])
    print(1-len(y_train[y_train==1])/len(y_train))
    # max_depths = np.arange(5,26,5)
    # accuracies = []
    # for depth in max_depths:
    #     tree = DTTree()
    #     tree.fit(X_train,y_train[:,0],types, max_depth = depth)
    #     Pred = tree.__call__(X_test,types)
    #     accuracies.append(len(np.where(Pred[:,0]==y_test[:,0])[0])/len(Pred))
    # print(accuracies)
    # plt.figure()
    # plt.plot(max_depths,accuracies)
    # plt.xlabel('Maximum depths')
    # plt.ylabel('Accuracies')
    # plt.title('Variation of Test set accuracy with max depth')
    # plt.show()
    plt.figure()
    plt.plot([15,25,35,45],[0.6324165761356485, 0.6484631233645543, 0.6534454214144575,   0.6611235313566458])
    plt.xlabel('Maximum depths')
    plt.ylabel('Accuracies')
    plt.title('Variation of Test set accuracy with max depth using post pruning')
    plt.show()


    OneHot = OneHotEncoder()
    inds=np.where(types=="cat")
    OneHot_X_train = OneHot.fit_transform(X_train.T[inds].T).toarray()
    OneHot_X_test = OneHot.fit_transform(X_test.T[inds].T).toarray()
    OneHot_X_val = OneHot.fit_transform(X_val.T[inds].T).toarray()

    OHtypes = ["cat"]*len(OneHot_X_train[0])
    for i in range(len(X_train[0])):
        if types[i]=="cont":
            OneHot_X_train = np.hstack([OneHot_X_train,np.reshape(X_train[:,i],(len(X_train[:,i]),1))])
            OneHot_X_test = np.hstack([OneHot_X_test,np.reshape(X_test[:,i],(len(X_test[:,i]),1))])
            OneHot_X_val = np.hstack([OneHot_X_val,np.reshape(X_val[:,i],(len(X_val[:,i]),1))])
            OHtypes.append("cont")

    max_depths = np.arange(15,46,1)
    accuracies = []
    for depth in max_depths:
        OHtree = DTTree()
        OHtree.fit(OneHot_X_train,y_train[:,0],OHtypes, max_depth = depth)
        Pred = OHtree.__call__(OneHot_X_test,OHtypes)
        accuracies.append(len(np.where(Pred==y_test[:,0])[0])/len(Pred))
    print(accuracies)
    plt.figure()
    plt.plot(max_depths,accuracies)
    plt.xlabel('Maximum depths')
    plt.ylabel('Accuracies in test set')
    plt.title('Variation of one hot accuracy with max depth')
    plt.show()

    max_depths = np.arange(15,46,10)
    accuracies = []
    for depth in max_depths:
        OHtree = DTTree()
        OHtree.fit(OneHot_X_train,y_train[:,0],OHtypes, max_depth = depth)
        OHtree.post_prune(OneHot_X_val, y_val,OHtypes,0.5)
        Pred_test = OHtree.__call__(OneHot_X_test,OHtypes)
        Pred_train = OHtree.__call__(OneHot_X_train,OHtypes)
        Pred_val = OHtree.__call__(OneHot_X_val,OHtypes)
        accuracies.append([len(np.where(Pred_test[:,0]==y_test[:,0])[0])/len(Pred_test),len(np.where(Pred_val[:,0]==y_val[:,0])[0])/len(Pred_val),len(np.where(Pred_train[:,0]==y_train[:,0])[0])/len(Pred_train)])
    accuracies = np.array(accuracies)
    print(accuracies[:,0])
    plt.figure()
    plt.plot(max_depths,accuracies[:,0])
    plt.xlabel('Maximum depths')
    plt.ylabel('Accuracies in test set')
    plt.title('Variation of One Hot accuracy with max depth using post prune')
    plt.show()
    print(accuracies[:,1])
    plt.figure()
    plt.plot(max_depths,accuracies[:,1])
    plt.xlabel('Maximum depths')
    plt.ylabel('Accuracies in validation set')
    plt.title('Variation of One Hot accuracy with max depth using post prune')
    plt.show()
    print(accuracies[:,2])
    plt.figure()
    plt.plot(max_depths,accuracies[:,2])
    plt.xlabel('Maximum depths')
    plt.ylabel('Accuracies in Train set')
    plt.title('Variation of One Hot accuracy with max depth using post prune')
    plt.show()

    max_depths = np.arange(15,46,1)
    accuracies = []
    for depth in max_depths:
        dtree = DecisionTreeClassifier(max_depth=depth,criterion="entropy",random_state=42)
        dtree.fit(X_train, y_train)
        Pred = dtree.predict(X_test)
        accuracies.append(len(np.where(Pred==y_test[:,0])[0])/len(Pred))
    print(accuracies)
    plt.figure()
    plt.plot(max_depths,accuracies)
    plt.xlabel('Maximum depths')
    plt.ylabel('Accuracies in test set')
    plt.title('Variation of sklearn accuracy with max depth')
    plt.show()
    best_depth = max_depths[np.argmax(np.array(accuracies))]

    CCP_alpha = [0.001, 0.01, 0.1, 0.2]
    accuracies = []
    for ccp in CCP_alpha:
        dtree = DecisionTreeClassifier(criterion="entropy",random_state=42,ccp_alpha=ccp)
        dtree.fit(X_train, y_train)
        Pred = dtree.predict(X_test)
        accuracies.append(len(np.where(Pred==y_test[:,0])[0])/len(Pred))
    print(accuracies)
    plt.figure()
    plt.plot(CCP_alpha,accuracies)
    plt.xlabel('pruning parameter')
    plt.ylabel('Accuracies in test set')
    plt.title('Variation of sklearn accuracy with pruning parameter')
    plt.show()
    best_prun_para = CCP_alpha[np.argmax(np.array(accuracies))]

    print(best_depth,best_prun_para)

    n_estimators = np.arange(50,351,100)
    accuracies = []
    for estim in n_estimators:
        Forest = RandomForestClassifier(n_estimators = estim, random_state=42)
        Forest.fit(X_train, y_train[:,0])
        Pred = Forest.predict(X_test)
        accuracies.append(len(np.where(Pred==y_test[:,0])[0])/len(Pred))
    print(accuracies)
    plt.figure()
    plt.plot(n_estimators,accuracies)
    plt.xlabel('N Estimator')
    plt.ylabel('Accuracies in test set')
    plt.title('Variation of sklearn accuracy with N Estimator')
    plt.show()

    max_samples = np.arange(0.1,1,0.2)
    accuracies = []
    for sample in max_samples:
        Forest = RandomForestClassifier(max_samples=sample, random_state=42)
        Forest.fit(X_train, y_train[:,0])
        Pred = Forest.predict(X_test)
        accuracies.append(len(np.where(Pred==y_test[:,0])[0])/len(Pred))
    print(accuracies)
    plt.figure()
    plt.plot(max_samples,accuracies)
    plt.xlabel('max samples')
    plt.ylabel('Accuracies in test set')
    plt.title('Variation of sklearn accuracy with max_samples')
    plt.show()

    min_samples_split = np.arange(2,11,2)
    accuracies = []
    for splitt in min_samples_split:
        Forest = RandomForestClassifier(min_samples_split=splitt, random_state=42)
        Forest.fit(X_train, y_train[:,0])
        Pred = Forest.predict(X_test)
        accuracies.append(len(np.where(Pred==y_test[:,0])[0])/len(Pred))
    print(accuracies)
    plt.figure()
    plt.plot(min_samples_split,accuracies)
    plt.xlabel(' min samples split')
    plt.ylabel('Accuracies in test set')
    plt.title('Variation of sklearn accuracy with min samples split')
    plt.show()

    param_grid = {
    'n_estimators': [50, 150, 250, 350],
    'max_features': [0.1, 0.3, 0.5, 0.7, 0.9, 1.0],
    'min_samples_split': [2, 4, 6, 8, 10]
    }
    Forest = RandomForestClassifier(oob_score=True, random_state=42)
    Grids = GridSearchCV(Forest, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
    Grids.fit(X_train, y_train)
    Best_para = Grids.best_params_
    Best_classifier = Grids.best_estimator_
    train_accuracy = Best_classifier.score(X_train, y_train)
    validation_accuracy = Best_classifier.score(X_val, y_val)
    test_accuracy = Best_classifier.score(X_test, y_test)

    print("Best Parameters:", Best_para)
    print("Train Accuracy:", train_accuracy)
    print("Validation Accuracy:", validation_accuracy)
    print("Test Accuracy:", test_accuracy)
    print("Out-of-Bag Accuracy (OOB):", Best_classifier.oob_score_)
